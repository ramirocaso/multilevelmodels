---
title: 'Assignment #1'
author: "Ramiro Cas√≥ - Incae Business School"
date: "6/8/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r 9_load_packages, include=FALSE}
rm(list=ls())
library(tidyverse)
library(lme4)
library(broom.mixed)
library(readr)
```

### Importing the data 

```{r}
data_wide <- read_csv("data_wide.csv")
View(data_wide)
```
### Task 1 Transpose the data from wide form to long form. 

Here is the code to pivot the data. I also preview the first 10 rows of the new pivoted data set

```{r}
data_long <- data_wide %>%
  pivot_longer(cols = c(y1,y2,y3,y4,y5,x1,x2,x3,x4,x5),
   names_to = c(".value", "Group"),
   names_pattern = "(.)(.)")

head(data_long,10)
```

### Task 2: Plot the data in aggregate and describe what you see 

```{r}
ggplot(data = data_long, mapping = aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", na.rm = T, col = "black", se = F)
```
What we can see from the data is a slightly negative relationship between x and y. 

### Task 3: Plot the data by group to see if there is any evidence of Simpson's paradox. What do you think after looking at this?

First, I'll try to see if coloring by group allow us to see if there is any evidence of Simpson's Paradox. 

```{r}
ggplot(data = data_long, mapping = aes(x = x, y = y)) +
  geom_point (na.rm = T, aes(col = Group), alpha = 0.5) +
  geom_smooth(method = "lm", na.rm = T, col = "black", se = F) +
  theme(legend.position = "top")

```
Since there are so many groups, it may be better to build separate scatter plots. 

```{r}
ggplot(data = data_long, mapping = aes(x = x, y = y)) +
     geom_point () +
     geom_smooth(method = "lm", na.rm = T, col = "black", se = F) +
     facet_wrap(~ Group) 
```


Yes, it appears to be evidence of Simpson's Paradox, since the relationship between x and y seems slightly positive for all groups, exept maybe for groups 3 and 5


### Task 4: Fit a pooled model ignoring the hierarchical structure of the data (using the lm function in r)


```{r}
pooled_model <- lm(y ~ 1 + x, data = data_long)
summary(pooled_model)
```

### Task 5: Fit a multilevel model with random intercepts for each group in the data (use the lmer function in r). What changes in the parameters between the pooled and multilevel models?

```{r}
# First, I need create a fit random intercepts model
intercepts_model <- lmer(y ~ 1+ x + (1 | Group), data = data_long) 

# Then I extract the group's coefficients. 
model_coefs_rani <- coef(intercepts_model)$Group %>% 
  rename(Intercept = `(Intercept)`, Slope = x) %>% 
  rownames_to_column("Group")

# and here we can see coefficients
model_coefs_rani
```
The main difference is in the sign of the slope. As you can see, in the pooled model, the slope is negative (as seen also in the original scatter plot). When you create a model with random intercepts, we now have a positive single value of slopes. This is evidence of Simpson's Paradox. 

We can plot this coefficients to see the difference by joining the new intercepts and slopes to the original data set. I will create a second data  set named `data_long_rani` for that. 

```{r}
data_long_rani <- left_join(data_long, model_coefs_rani, by = "Group")
```

Once I have extracted the coefficients and joined them to my original data set, I can plot it.

```{r}
model_coef_plot <- ggplot(data = data_long_rani, 
       mapping = aes(x = x, 
                     y = y, 
                     colour = Group)
       ) +
  geom_point(na.rm = T, alpha = 0.5) +
  geom_abline(aes(intercept = Intercept, 
                  slope = Slope,
                  colour = Group
                  ),
              size = 1.5
              ) +
  theme(legend.position = "top")

# see the plot
model_coef_plot
```


### Task 6: Fit a model with random intercepts and slopes for each group. 

```{r}
# fit random slopes model
ranis_model <- lmer(y ~ x + (1 + x | Group), data = data_long) 

# Then I extract the group's coefficients as before
model_coefs_ranis <- coef(ranis_model)$Group %>% 
  rename(Intercept = `(Intercept)`, Slope = x) %>% 
  rownames_to_column("Group")

# see coefficients
model_coefs_ranis
```

Next, we'll join these coefficients to our original data.

```{r}
data_long_ranis <- left_join(data_long, model_coefs_ranis, by = "Group")
```

Then we'll plot our original data with our new, random intercepts model. 

```{r}
model_coef_plot %+% data_long_ranis
```

### Task 7: Fit a model with crossed random effects (both intercepts and slopes) for both group and id.

```{r}
cross_ran_model <- lmer(y ~ 1+ x + (1 | id) + (1 | Group), data = data_long)
summary(cross_ran_model)
```

### Task 8: Because the models in Tasks 5, 6, and 7 are all nested, use a Chi-square test on model deviance to assess whether the more complicated models (i.e., those in Tasks 6 and 7) provide better model fits. To do this, it would be helpful to use the "anova" function in r.

```{r}
anova(intercepts_model, ranis_model, cross_ran_model, test = "Chisq")
```

If I recall correctly, one should choose the model with lowest BIC. In this case, it appears that the random intercept model is the one that best fits the data. 

### Task 9: [Not graded because there is a chance you won't figure it out in r] get the predicted values (i.e., y_hat) for the model from Task 7. Then plot x and the predicted y's for the first 5 id's in the dataset also accounting for the 5 groups. 

```{r}
predicted_values <- modelr:: data_grid(subset(data_long, id<=5), x, Group, id) %>%
  modelr::add_predictions(cross_ran_model)
```

Now I plot the predicted values. 

```{r}
predicted_values %>%
ggplot(aes(x = x, y = pred, colour = as.factor(id), shape = as.factor(Group) )) +
  geom_point() 
```



Plotting the values of X and Yhat shows a slightly positive relation, that is, higher values for X will result in high values of Yhat. Looking at the first 5 id's, it seams like there is a notable differences between the groups (intercepts), with group 5 seems to have the lowest predicted values and Group 1 the highest. 
